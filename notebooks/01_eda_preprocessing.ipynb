{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Exploratory Data Analysis and Preprocessing\n",
    "\n",
    "This notebook covers:\n",
    "1. Dataset description and origin\n",
    "2. Data loading and initial exploration\n",
    "3. Data quality assessment\n",
    "4. Handling missing values and outliers\n",
    "5. Exploratory Data Analysis (distributions, correlations)\n",
    "6. Feature preparation for unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.config_loader import load_config, get_absolute_path\n",
    "from src.data_loading import DataLoader\n",
    "from src.preprocessing import DataPreprocessor\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Description\n",
    "\n",
    "### Origin and Context\n",
    "**Dataset Name**: [Fill in dataset name]  \n",
    "**Source**: [Describe where the data comes from]  \n",
    "**Domain**: [Business/Government/Health/Environment]  \n",
    "**Collection Method**: [How was the data collected?]  \n",
    "**Time Period**: [When was the data collected?]  \n",
    "\n",
    "### Purpose\n",
    "[Describe the purpose of this analysis and what questions you want to answer]\n",
    "\n",
    "### Expected Structure\n",
    "- **Rows**: ≥1000 samples\n",
    "- **Columns**: ≥10 variables\n",
    "- **Variable Types**: Mix of numerical and potentially categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config()\n",
    "\n",
    "# Initialize data loader\n",
    "loader = DataLoader(config)\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    data = loader.load_data()\n",
    "    print(f\"✓ Data loaded successfully!\")\n",
    "    print(f\"Shape: {data.shape}\")\n",
    "    print(f\"Columns: {list(data.columns)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Please place 'base_historica.csv' in the data/raw/ directory\")\n",
    "    data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data is not None:\n",
    "    print(\"Dataset Overview:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Number of rows: {data.shape[0]}\")\n",
    "    print(f\"Number of columns: {data.shape[1]}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    display(data.head())\n",
    "    \n",
    "    print(f\"\\nData Types:\")\n",
    "    print(data.dtypes)\n",
    "    \n",
    "    print(f\"\\nBasic Statistics:\")\n",
    "    display(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Assessment\n",
    "\n",
    "### 4.1 Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data is not None:\n",
    "    missing_values = data.isnull().sum()\n",
    "    missing_percentage = (missing_values / len(data)) * 100\n",
    "    \n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing_values,\n",
    "        'Percentage': missing_percentage\n",
    "    }).sort_values('Percentage', ascending=False)\n",
    "    \n",
    "    print(\"Missing Values Summary:\")\n",
    "    display(missing_df[missing_df['Missing Count'] > 0])\n",
    "    \n",
    "    # Visualize missing values\n",
    "    if missing_df['Missing Count'].sum() > 0:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        missing_data = missing_df[missing_df['Missing Count'] > 0]\n",
    "        plt.barh(missing_data.index, missing_data['Percentage'])\n",
    "        plt.xlabel('Percentage of Missing Values')\n",
    "        plt.title('Missing Values by Column')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"✓ No missing values found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Duplicates Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data is not None:\n",
    "    n_duplicates = data.duplicated().sum()\n",
    "    print(f\"Number of duplicate rows: {n_duplicates}\")\n",
    "    if n_duplicates > 0:\n",
    "        print(f\"Percentage: {n_duplicates/len(data)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 5.1 Numerical Features Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data is not None:\n",
    "    # Identify numerical columns\n",
    "    numerical_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    print(f\"Number of numerical features: {len(numerical_cols)}\")\n",
    "    print(f\"Numerical features: {numerical_cols}\")\n",
    "    \n",
    "    if len(numerical_cols) > 0:\n",
    "        # Distribution plots\n",
    "        n_cols = 3\n",
    "        n_rows = (len(numerical_cols) + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "        axes = axes.flatten() if n_rows > 1 else [axes]\n",
    "        \n",
    "        for idx, col in enumerate(numerical_cols):\n",
    "            if idx < len(axes):\n",
    "                data[col].hist(bins=30, ax=axes[idx], edgecolor='black')\n",
    "                axes[idx].set_title(f'Distribution of {col}')\n",
    "                axes[idx].set_xlabel(col)\n",
    "                axes[idx].set_ylabel('Frequency')\n",
    "        \n",
    "        # Hide extra subplots\n",
    "        for idx in range(len(numerical_cols), len(axes)):\n",
    "            axes[idx].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data is not None and len(numerical_cols) > 0:\n",
    "    # Compute correlation matrix\n",
    "    correlation = data[numerical_cols].corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Correlation Heatmap of Numerical Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify highly correlated pairs\n",
    "    print(\"\\nHighly Correlated Feature Pairs (|correlation| > 0.7):\")\n",
    "    high_corr = []\n",
    "    for i in range(len(correlation.columns)):\n",
    "        for j in range(i+1, len(correlation.columns)):\n",
    "            if abs(correlation.iloc[i, j]) > 0.7:\n",
    "                high_corr.append((correlation.columns[i], correlation.columns[j], correlation.iloc[i, j]))\n",
    "    \n",
    "    if high_corr:\n",
    "        for feat1, feat2, corr in high_corr:\n",
    "            print(f\"  {feat1} <-> {feat2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(\"  No highly correlated pairs found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data is not None and len(numerical_cols) > 0:\n",
    "    # Create box plots for outlier detection\n",
    "    n_cols = 3\n",
    "    n_rows = (len(numerical_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes]\n",
    "    \n",
    "    for idx, col in enumerate(numerical_cols):\n",
    "        if idx < len(axes):\n",
    "            data.boxplot(column=col, ax=axes[idx])\n",
    "            axes[idx].set_title(f'Box Plot of {col}')\n",
    "            axes[idx].set_ylabel(col)\n",
    "    \n",
    "    # Hide extra subplots\n",
    "    for idx in range(len(numerical_cols), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Categorical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data is not None:\n",
    "    # Identify categorical columns\n",
    "    categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    print(f\"Number of categorical features: {len(categorical_cols)}\")\n",
    "    print(f\"Categorical features: {categorical_cols}\")\n",
    "    \n",
    "    if len(categorical_cols) > 0:\n",
    "        # Show unique values for each categorical column\n",
    "        for col in categorical_cols:\n",
    "            n_unique = data[col].nunique()\n",
    "            print(f\"\\n{col}: {n_unique} unique values\")\n",
    "            if n_unique <= 10:\n",
    "                print(data[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing\n",
    "\n",
    "### 6.1 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data is not None:\n",
    "    # Initialize preprocessor\n",
    "    preprocessor = DataPreprocessor(scaling_method='standard')\n",
    "    \n",
    "    # Handle missing values\n",
    "    data_clean = preprocessor.handle_missing_values(\n",
    "        data.copy(), \n",
    "        strategy='median', \n",
    "        threshold=0.5\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Missing values handled\")\n",
    "    print(f\"Shape after handling missing values: {data_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data is not None:\n",
    "    # Remove outliers using IQR method\n",
    "    data_no_outliers = preprocessor.remove_outliers(\n",
    "        data_clean.copy(),\n",
    "        method='iqr',\n",
    "        threshold=3.0\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Outliers removed\")\n",
    "    print(f\"Shape after removing outliers: {data_no_outliers.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data is not None:\n",
    "    # Encode categorical variables\n",
    "    data_encoded = preprocessor.encode_categorical(\n",
    "        data_no_outliers.copy(),\n",
    "        method='onehot'\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Categorical variables encoded\")\n",
    "    print(f\"Shape after encoding: {data_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data is not None:\n",
    "    # Scale features\n",
    "    data_scaled = preprocessor.scale_features(data_encoded.copy(), fit=True)\n",
    "    \n",
    "    print(f\"✓ Features scaled\")\n",
    "    print(f\"Final shape: {data_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data is not None:\n",
    "    # Save processed data\n",
    "    output_path = get_absolute_path('data/processed/processed_data.csv')\n",
    "    data_scaled.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"✓ Processed data saved to: {output_path}\")\n",
    "    print(f\"\\nFinal dataset ready for unsupervised learning:\")\n",
    "    print(f\"  - Rows: {data_scaled.shape[0]}\")\n",
    "    print(f\"  - Features: {data_scaled.shape[1]}\")\n",
    "    print(f\"  - All numerical: {data_scaled.select_dtypes(include=[np.number]).shape[1] == data_scaled.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "### Key Findings from EDA:\n",
    "1. [Finding 1]\n",
    "2. [Finding 2]\n",
    "3. [Finding 3]\n",
    "\n",
    "### Preprocessing Steps Applied:\n",
    "- Missing values: [strategy]\n",
    "- Outliers: [method and threshold]\n",
    "- Categorical encoding: [method]\n",
    "- Feature scaling: [method]\n",
    "\n",
    "### Next Steps:\n",
    "- Proceed to dimensionality reduction (Notebook 02)\n",
    "- Apply PCA and UMAP\n",
    "- Visualize data in reduced dimensions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
